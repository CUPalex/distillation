## Дистилляция нейронный сетей для распознавания намерений пользователя
Дистилляция модели SBERT-Large, предобученной на корпусе текстов на русском языке, в BiLSTM, и дообучение до задачи классификации. Дистиллированная модель имеет 9М параметров и f1-score 0.67.
### Структура репозитория
 - **models** - сохраненные обученные модели.
 - **notebooks** - .ipynb ноутбуки, в которых дистиллируется модель. Ноутбуки пронумерованы и должны запускаться последовательно.
### Описание запуска программы
Для того, чтобы заново дистиллировать и дообучить модель, достаточно загрузить данные для обучения к себе на Google Disk, открыть тетрадки из папки notebooks в Google Colab, изменить пути к файлам с данными относительно их расположения на диске и запустить последовательно сначала первую, а затем вторую тетрадки. Можно запускать не в Google Colab, достаточно иметь доступ к Интернету и правильно указать пути к файлам.
Для того, чтобы загрузить дообученную модель в свой код, установите библиотеку transformers, pytorh, скопируйте из ноутбука 2_distilled_SBERT_fine_tune класс Net, загрузите sbertTokenizer из библиотеки transformers и запустите следующие строки:

    from transformers import BertTokenizerFast
    model = Net()
    model.load_state_dict(torch.load('distilled-model')
    model.to(device)

После чего можно использовать model как готовую модель. Примеры загрузки и использования модели можно посмотреть в ноутбуке usage-example.