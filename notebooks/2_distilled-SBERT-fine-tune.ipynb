{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distilled SBERT-fine-tune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClqewEEEh8Hd"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6_fHm2vmcyq"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w32QtfcQh_sg",
        "outputId": "805668b4-8fb9-42fd-eadb-fc43f180d37a"
      },
      "source": [
        "# Использовать gpu\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QZtTAhbiC0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b6f051-0f98-4a0e-d611-c0ac53ecf77a"
      },
      "source": [
        "!pip install transformers\n",
        "!apt-get install unzip wget -y"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W98yFjQ3iJmd"
      },
      "source": [
        "PATH_TO_DISK = '/content/drive'"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRjNOmM6iDZL",
        "outputId": "bb182989-3fc5-4d1f-9086-b688211dc710"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(PATH_TO_DISK)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFAlEAMkg3c"
      },
      "source": [
        "## Download pretrained model and pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97EEW0KGk-UC"
      },
      "source": [
        "PATH_TO_MODEL = os.path.join(PATH_TO_DISK, 'My Drive', 'Colab Notebooks/15_epoch_simple_lstm_100.pt')\n",
        "\n",
        "PATH_TO_PRETRAINED_TOKENIZER = 'sberbank-ai/sbert_large_nlu_ru'"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zAdGmwbqBN-"
      },
      "source": [
        "# Загрузить sbertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "sbertTokenizer = AutoTokenizer.from_pretrained(PATH_TO_PRETRAINED_TOKENIZER)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH0zU9FMkmoD"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Класс дистиллированной модели-ученика, которую мы будем файнтюнить.\n",
        "# Нужно скопировать из тетрадки, в которой модель дистиллировалась\n",
        "class SimpleLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, batch_size, device=None):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.device = self.init_device(device)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    @staticmethod\n",
        "    def init_device(device):\n",
        "        if device is None:\n",
        "            return torch.device('cuda')\n",
        "        return device\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(2 * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)),\n",
        "                Variable(torch.zeros(2 * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)))\n",
        "\n",
        "    def forward(self, text, text_lengths=None):\n",
        "        self.hidden = self.init_hidden()\n",
        "        x = self.embedding(text)\n",
        "        x, self.hidden = self.rnn(x, self.hidden)\n",
        "        hidden, cell = self.hidden\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        x = self.fc(hidden)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMmOvpibb9Kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8da7108-bff6-49cd-8546-1ac900afdf22"
      },
      "source": [
        "# Загружаем дистиллированную модель. Все параметры должны быть перенесены из тетрадки с дистилляцией.\n",
        "modelParameters = {'input_dim' : sbertTokenizer.vocab_size,\n",
        "                   'embedding_dim' : 50,\n",
        "                   'hidden_dim' : 256,\n",
        "                   'output_dim' : 1024,\n",
        "                   'n_layers' : 2,\n",
        "                   'bidirectional' : True,\n",
        "                   'dropout' : 0.2,\n",
        "                   'batch_size' : 32}\n",
        "\n",
        "model = SimpleLSTM(\n",
        "            input_dim = modelParameters['input_dim'],\n",
        "            embedding_dim = modelParameters['embedding_dim'],\n",
        "            hidden_dim = modelParameters['hidden_dim'],\n",
        "            output_dim = modelParameters['output_dim'],\n",
        "            n_layers = modelParameters['n_layers'],\n",
        "            bidirectional = modelParameters['bidirectional'],\n",
        "            dropout = modelParameters['dropout'],\n",
        "            batch_size = modelParameters['batch_size'])\n",
        "\n",
        "model.load_state_dict(torch.load(PATH_TO_MODEL))\n",
        "\n",
        "# Переносим модель на GPU\n",
        "model.to(device)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleLSTM(\n",
              "  (embedding): Embedding(120138, 50)\n",
              "  (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDz1sPxbcDXX",
        "outputId": "511aa9d3-32e3-4426-919b-ade0c28563a6"
      },
      "source": [
        "# Проверим, что модель правильно загрузилась\n",
        "tokenized_sent = sbertTokenizer(['Инфляция - жуткая штука'] *32 , padding=True, truncation=True, max_length=20)\n",
        "inds = torch.tensor(tokenized_sent['input_ids'])\n",
        "inds_cuda = inds.to(device)\n",
        "model.eval()\n",
        "model(inds_cuda.t(), None)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        ...,\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L7o03DHWjzE"
      },
      "source": [
        "# Prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3Ur99khiRAC"
      },
      "source": [
        "# Путь к файлу с данными в .csv формате.\n",
        "# Здесь мы используем датасет Lenta.ru\n",
        "PATH_TO_DATA = os.path.join(PATH_TO_DISK, 'My Drive', 'Colab Notebooks/test-lenta.csv')\n",
        "\n",
        "# Если данные в .zip файле, предварительно их нужно распаковать и указать путь на распакованный .csv-файл\n",
        "# !unzip -o PATH_TO_DATA\n",
        "# PATH_TO_DATA = ...\n",
        "\n",
        "TOKENIZER_MAX_LENGHT = 20"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0nrDS8pm8IK"
      },
      "source": [
        "def getPreparedDataFromCSV(path, max_lenght):\n",
        "    '''\n",
        "    Принимает:  path - путь к .csv файлу с данными\n",
        "                max_lenght - параметр для токенизатора\n",
        "    Выдает:     input_ids - токенизированные предложения, torch.tensor\n",
        "                labels - класссы каждого предложения, которые мы должны научиться предсказывать, torch.tensor\n",
        "                id_to_topic - соответствие классов и реальных меток, list\n",
        "    '''\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    topic_to_id = {}\n",
        "    id_to_topic = []\n",
        "    for topic in data.topic:\n",
        "        if not topic in topic_to_id:\n",
        "            topic_to_id[topic] = len(id_to_topic)\n",
        "            id_to_topic.append(topic)\n",
        "\n",
        "    labels = []\n",
        "    for topic in data.topic:\n",
        "        labels.append(topic_to_id[topic])\n",
        "    \n",
        "    input_ids = [] # encoded sentences\n",
        "\n",
        "    for sent in data.title:\n",
        "        encoded_input = sbertTokenizer(sent, padding='max_length',\n",
        "                                       truncation=True,\n",
        "                                       max_length=max_lenght,\n",
        "                                       return_tensors='pt')\n",
        "        input_ids.append(encoded_input['input_ids'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, labels, id_to_topic"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5NyQViWANnj",
        "outputId": "24f1088e-8636-4c63-d78b-c1dca44796b9"
      },
      "source": [
        "# Прочитаем данные из файла и рандомно разобьем их на датасеты\n",
        "# в отношении 7 : 2 : 1 для train : validation : test\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "input_ids, labels, id_to_topic = getPreparedDataFromCSV(PATH_TO_DATA, TOKENIZER_MAX_LENGHT)\n",
        "dataset = TensorDataset(input_ids, labels)\n",
        "\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} testing samples'.format(test_size))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35,000 training samples\n",
            "10,000 validation samples\n",
            "5,000 testing samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkk3y_z_Fsep"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyc8ho17gdNv"
      },
      "source": [
        "trainingParameters = {\n",
        "    'optimizer' : {\n",
        "        'lr' : 2e-5,\n",
        "        'eps' : 1e-8\n",
        "    },\n",
        "    'epochs' : 30,\n",
        "    'batch_size' : modelParameters['batch_size']\n",
        "}"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oFeaUsBuXh3"
      },
      "source": [
        "# Создадим DataLoader для каждого датасета\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = trainingParameters['batch_size']\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "testing_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = SequentialSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBILkBfKGLBe"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = trainingParameters['optimizer']['lr'],\n",
        "                  eps = trainingParameters['optimizer']['eps']\n",
        "                )\n"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tz1Pej3HwYZ"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = trainingParameters['epochs']\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r3Ja7qzIJLn"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwFf7BgJLnIA"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgqnctvOmgyC"
      },
      "source": [
        "# Инициализируем модель, которую будем тренировать\n",
        "# Здесь можно поиграть с количеством слоев и внутренних параметров\n",
        "# Я пробовала один линейный слой и число внутренних параметров (512, 512), (1024, 512), (512, 128)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.model = model\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.lin1 = nn.Linear(1024, 512)\n",
        "        self.lin2 = nn.Linear(512, 256)\n",
        "        self.lin3 = nn.Linear(256, len(id_to_topic))\n",
        "\n",
        "    def forward(self, x, text_lengths=None):\n",
        "        x = self.model(x.t(), text_lengths=text_lengths).squeeze(1)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "        return x"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD3iBnkOq14k",
        "outputId": "8a7b48d2-2073-454a-8f7d-64da717a26d3"
      },
      "source": [
        "net = Net()\n",
        "net.to(device)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (model): SimpleLSTM(\n",
              "    (embedding): Embedding(120138, 50)\n",
              "    (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lin1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (lin3): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syqaDKOELyKa",
        "outputId": "7301c2ca-08a2-4a06-cea4-d0333d9c9f26"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# We'll store validation loss, validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print()\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if len(batch[0]) != batch_size:\n",
        "            continue\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_labels = batch[1].cuda()\n",
        "\n",
        "        net.zero_grad()\n",
        "        optimizer.zero_grad()  \n",
        "\n",
        "        result = net(b_input_ids)\n",
        "\n",
        "        loss = criterion(result, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        del b_input_ids\n",
        "        del b_labels\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        if len(batch[0]) != batch_size:\n",
        "            continue\n",
        "    \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            result = net(b_input_ids)\n",
        "\n",
        "        loss = criterion(result, b_labels)\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        _, predicted = torch.max(result.data, 1)\n",
        "        total += b_labels.size(0)\n",
        "        correct += (predicted == b_labels).sum().item()\n",
        "\n",
        "        del b_input_ids\n",
        "        del b_labels\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "\n",
        "\n",
        "    avg_val_accuracy = correct / total\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 2.25\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.34\n",
            "  Validation Loss: 2.14\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.99\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.47\n",
            "  Validation Loss: 1.75\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.68\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 1.53\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.50\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.54\n",
            "  Validation Loss: 1.42\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.40\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.36\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.33\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.31\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.28\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation Loss: 1.27\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.24\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation Loss: 1.25\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.21\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.23\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.21\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.19\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.13\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.18\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.11\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.17\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.09\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.16\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.08\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.15\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 16 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.15\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 17 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.14\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 18 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.03\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.14\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 19 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.02\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 20 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.02\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 21 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 22 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.00\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 23 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 24 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 25 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 26 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 27 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 28 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.97\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 29 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.98\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 30 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 0.97\n",
            "  Training epcoh took: 0:00:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:07:02 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2mr9A_Gb1qz"
      },
      "source": [
        "## Display statictics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m8qFEazDNFXv",
        "outputId": "e2c85148-67a8-4b0d-a83a-0e849ff70c9e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "df = df_stats.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "df_stats"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.25</td>\n",
              "      <td>2.14</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.99</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.68</td>\n",
              "      <td>1.53</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.50</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.40</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.33</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.28</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.24</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.21</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.17</td>\n",
              "      <td>1.21</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.15</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.13</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.11</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.09</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.08</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.06</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.05</td>\n",
              "      <td>1.14</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.03</td>\n",
              "      <td>1.14</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.02</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.02</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.01</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.99</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.99</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.99</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.98</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.98</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.97</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.98</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.97</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:13</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               2.25         2.14           0.34       0:00:13         0:00:01\n",
              "2               1.99         1.75           0.47       0:00:13         0:00:01\n",
              "3               1.68         1.53           0.52       0:00:13         0:00:01\n",
              "4               1.50         1.42           0.54       0:00:13         0:00:01\n",
              "5               1.40         1.36           0.56       0:00:13         0:00:01\n",
              "6               1.33         1.31           0.57       0:00:13         0:00:01\n",
              "7               1.28         1.27           0.59       0:00:13         0:00:01\n",
              "8               1.24         1.25           0.59       0:00:13         0:00:01\n",
              "9               1.21         1.23           0.60       0:00:13         0:00:01\n",
              "10              1.17         1.21           0.60       0:00:13         0:00:01\n",
              "11              1.15         1.19           0.61       0:00:13         0:00:01\n",
              "12              1.13         1.18           0.61       0:00:13         0:00:01\n",
              "13              1.11         1.17           0.62       0:00:13         0:00:01\n",
              "14              1.09         1.16           0.62       0:00:13         0:00:01\n",
              "15              1.08         1.15           0.62       0:00:13         0:00:01\n",
              "16              1.06         1.15           0.62       0:00:13         0:00:01\n",
              "17              1.05         1.14           0.63       0:00:13         0:00:01\n",
              "18              1.03         1.14           0.62       0:00:13         0:00:01\n",
              "19              1.02         1.13           0.63       0:00:13         0:00:01\n",
              "20              1.02         1.13           0.63       0:00:13         0:00:01\n",
              "21              1.01         1.12           0.63       0:00:13         0:00:01\n",
              "22              1.00         1.12           0.63       0:00:13         0:00:01\n",
              "23              0.99         1.12           0.63       0:00:13         0:00:01\n",
              "24              0.99         1.12           0.63       0:00:13         0:00:01\n",
              "25              0.99         1.12           0.63       0:00:13         0:00:01\n",
              "26              0.98         1.12           0.63       0:00:13         0:00:01\n",
              "27              0.98         1.11           0.63       0:00:13         0:00:01\n",
              "28              0.97         1.11           0.63       0:00:13         0:00:01\n",
              "29              0.98         1.11           0.63       0:00:13         0:00:01\n",
              "30              0.97         1.11           0.63       0:00:13         0:00:01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CutE55T6L4gr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "1d88d93f-ad51-4dd4-ff70-32caf7009f53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNZ1BkfrK7PZ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDZlNbOKNeFd",
        "outputId": "3d2637e6-1807-45ce-cf6b-8559dbeac0d1"
      },
      "source": [
        "net.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "print()\n",
        "print('Testing...')\n",
        "\n",
        "for (step, batch) in enumerate(testing_dataloader):\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    if len(batch[0]) != batch_size:\n",
        "            continue\n",
        "    \n",
        "    if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(testing_dataloader)))\n",
        "\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        result = net(b_input_ids)\n",
        "\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    _, predicted = torch.max(result.data, 1)\n",
        "\n",
        "    predictions.append(predicted)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "    del b_input_ids\n",
        "    del b_labels\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print('DONE.')"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Testing...\n",
            "  Batch    40  of    157.\n",
            "  Batch    80  of    157.\n",
            "  Batch   120  of    157.\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDb2mTW3Ufm3",
        "outputId": "3d526e82-37e5-4203-d7fa-b1c0dc5ea412"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "predicted_labels = np.concatenate([np.array(prediction.to('cpu'), dtype='long') for prediction in predictions])\n",
        "\n",
        "y_true = np.concatenate(true_labels).flatten()\n",
        "y_pred = predicted_labels.flatten()\n",
        "\n",
        "print(\"Precision: {0:.4f}\".format(precision_score(y_true, y_pred, average='macro'))) # unweighted average of precisions\n",
        "print(\"Recall: {0:.4f}\".format(recall_score(y_true, y_pred, average='macro'))) # unweighted average of recalls\n",
        "print(\"F1-score: {0:.4f}\".format(f1_score(y_true, y_pred, average='macro'))) # unweighted average of f1-scores"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.6394\n",
            "Recall: 0.6378\n",
            "F1-score: 0.6376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugSLY13qWJ55"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltf0COQohwRW"
      },
      "source": [
        "OUTPUT_DIR = os.path.join(PATH_TO_DISK, 'My Drive')\n",
        "MODEL_NAME_TO_SAVE = 'distilled-model'\n",
        "PARAMETERS_NAME_TO_SAVE = 'args-distilled-model.bin'"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4DxhvNRcfb"
      },
      "source": [
        "# Save model, tokenizer and training arguments\n",
        "torch.save(net.state_dict(), os.path.join(OUTPUT_DIR, MODEL_NAME_TO_SAVE))\n",
        "sbertTokenizer.save_pretrained(OUTPUT_DIR)\n",
        "torch.save(trainingParameters, os.path.join(OUTPUT_DIR, PARAMETERS_NAME_TO_SAVE))"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNekoZXbvQU"
      },
      "source": [
        "Look at the sizes, out of curiousity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY8TekbTWsbn",
        "outputId": "a8023744-1e23-4a4c-d428-af9ad4e7473b"
      },
      "source": [
        "output_dir = './model_save'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "torch.save(net.state_dict(), os.path.join(output_dir, MODEL_NAME_TO_SAVE))\n",
        "sbertTokenizer.save_pretrained(output_dir)\n",
        "torch.save(trainingParameters, os.path.join(output_dir, PARAMETERS_NAME_TO_SAVE))\n",
        "\n",
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 41164K\n",
            "-rw-r--r-- 1 root root     1K May 27 14:30 args-distilled-model.bin\n",
            "-rw-r--r-- 1 root root 36719K May 27 14:30 distilled-model\n",
            "-rw-r--r-- 1 root root     1K May 27 14:30 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root     1K May 27 14:30 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  2689K May 27 14:30 tokenizer.json\n",
            "-rw-r--r-- 1 root root  1739K May 27 14:30 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-I1RKpGXGZT"
      },
      "source": [
        "To load model from drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiYeAnC5SbSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae33c1a-8f96-4629-e4eb-b83bcb5e86fa"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Net class must be the same as class of the saved model. Initialize it somewehere before\n",
        "model = Net()\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, MODEL_NAME_TO_SAVE)))\n",
        "tokenizer = BertTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (model): SimpleLSTM(\n",
              "    (embedding): Embedding(120138, 50)\n",
              "    (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lin1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (lin3): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8c3T1gFcwSS"
      },
      "source": [
        "Done :)"
      ]
    }
  ]
}