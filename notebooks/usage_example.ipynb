{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "usage-example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C1nuG56tY0g"
      },
      "source": [
        "# Демонстация использования модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MawKA7tltjBu"
      },
      "source": [
        "### Подготовим среду выполнения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dQa4eLJcF5j",
        "outputId": "9ee8bae0-ed88-48b8-be37-00fdd7a694d0"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YadZr0O2mzrB",
        "outputId": "e2238e99-bfeb-4d9f-ca0f-14f913f12f72"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKk2uAdwoXha",
        "outputId": "9cc09b2c-676f-412d-f3cb-f78577805c7d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JTTgeERtn2H"
      },
      "source": [
        "### Загрузим токенайзер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOJa3H4yoSJL"
      },
      "source": [
        "PATH_TO_PRETRAINED_TOKENIZER = 'sberbank-ai/sbert_large_nlu_ru'\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "sbertTokenizer = AutoTokenizer.from_pretrained(PATH_TO_PRETRAINED_TOKENIZER)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIK3q3Eltunp"
      },
      "source": [
        "### Объявим класс дистиллированной модели и загрузим модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pfm4saeln0z"
      },
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, batch_size, device=None):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.device = self.init_device(device)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    @staticmethod\n",
        "    def init_device(device):\n",
        "        if device is None:\n",
        "            return torch.device('cuda')\n",
        "        return device\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(2 * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)),\n",
        "                Variable(torch.zeros(2 * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)))\n",
        "\n",
        "    def forward(self, text, text_lengths=None):\n",
        "        self.hidden = self.init_hidden()\n",
        "        x = self.embedding(text)\n",
        "        x, self.hidden = self.rnn(x, self.hidden)\n",
        "        hidden, cell = self.hidden\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        x = self.fc(hidden)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0GUNU0clvdW"
      },
      "source": [
        "modelParameters = {'input_dim' : sbertTokenizer.vocab_size,\n",
        "                   'embedding_dim' : 50,\n",
        "                   'hidden_dim' : 256,\n",
        "                   'output_dim' : 1024,\n",
        "                   'n_layers' : 2,\n",
        "                   'bidirectional' : True,\n",
        "                   'dropout' : 0.2,\n",
        "                   'batch_size' : 32}\n",
        "\n",
        "model = SimpleLSTM(\n",
        "            input_dim = modelParameters['input_dim'],\n",
        "            embedding_dim = modelParameters['embedding_dim'],\n",
        "            hidden_dim = modelParameters['hidden_dim'],\n",
        "            output_dim = modelParameters['output_dim'],\n",
        "            n_layers = modelParameters['n_layers'],\n",
        "            bidirectional = modelParameters['bidirectional'],\n",
        "            dropout = modelParameters['dropout'],\n",
        "            batch_size = modelParameters['batch_size'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY3MFXIsd1zF"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DistilledModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilledModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.lin = nn.Linear(1024, 10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x, text_lengths=None):\n",
        "        x = self.model(x.t(), text_lengths=text_lengths).squeeze(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v5NYgPzmKZE",
        "outputId": "a0653504-3868-4d53-fea1-87c961d98952"
      },
      "source": [
        "PATH_TO_MODEL = '/content/drive/My Drive/distilled-model'\n",
        "\n",
        "distilledModel = DistilledModel()\n",
        "\n",
        "distilledModel.load_state_dict(torch.load(PATH_TO_MODEL))\n",
        "\n",
        "distilledModel.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilledModel(\n",
              "  (model): SimpleLSTM(\n",
              "    (embedding): Embedding(120138, 50)\n",
              "    (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lin): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXpR3007t2Wn"
      },
      "source": [
        "### Предскажем темы для некоторых предложений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaZTpimvtUgF"
      },
      "source": [
        "# Этот массив должен быть именно таким. В случае, если вы хотите предсказывать другие темы, необходимо заново дообучить модель в другой тетрадке.\n",
        "id_to_topic = ['Из жизни','Россия','Наука и техника','Культура','Бывший СССР','Экономика','Бизнес','Спорт','Путешествия','Интернет и СМИ']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYZnOvNPmc25"
      },
      "source": [
        "def predict(model, tokenizer, sent, id_to_topic):\n",
        "        tokenized_sent = tokenizer([sent] * 32, padding=True, truncation=True, max_length=20)\n",
        "        inds = torch.tensor(tokenized_sent['input_ids'])\n",
        "        inds_cuda = inds.to(device)\n",
        "        model.eval()\n",
        "        model_output = model(inds_cuda, None)[0]\n",
        "        _, predicted = torch.max(model_output, 0)\n",
        "        return id_to_topic[predicted]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXXgw_D9q_eI"
      },
      "source": [
        "sents = [\n",
        "         'Инфляция - жуткая штука',\n",
        "         'В Большом театре без перемен',\n",
        "         'Суперкомпьютеры определяют будущее',\n",
        "         'Футболисты выиграли матч'\n",
        "]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRejd2BBqvKJ",
        "outputId": "1cb49b62-0e01-4c92-a27f-a4f8cfaac252"
      },
      "source": [
        "for sent in sents:\n",
        "    predicted = predict(distilledModel, sbertTokenizer, sent, id_to_topic)\n",
        "    print('Предложение \\'%s\\'  имеет тему \\'%s\\'' % (sent, predicted))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Предложение 'Инфляция - жуткая штука'  имеет тему 'Экономика'\n",
            "Предложение 'В Большом театре без перемен'  имеет тему 'Культура'\n",
            "Предложение 'Суперкомпьютеры определяют будущее'  имеет тему 'Наука и техника'\n",
            "Предложение 'Футболисты выиграли матч'  имеет тему 'Спорт'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfLWRwWnmoq7",
        "outputId": "110dfc34-0f24-43ed-fc1e-3068b41845d6"
      },
      "source": [
        "tokenized_sent = sbertTokenizer('Футболисты выиграли матч', padding=True, truncation=True, max_length=20); tokenized_sent"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 20236, 19672, 5071, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5GUIReFmvK5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}