{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distilled SBERT-fine-tune.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClqewEEEh8Hd"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6_fHm2vmcyq"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w32QtfcQh_sg",
        "outputId": "dbee20f2-aad8-4ce5-fcf1-120ea4132c13"
      },
      "source": [
        "# Использовать gpu\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QZtTAhbiC0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4617548b-3b97-4c29-a6c3-fa20bc1c2cc5"
      },
      "source": [
        "!pip install transformers\n",
        "!apt-get install unzip wget -y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W98yFjQ3iJmd"
      },
      "source": [
        "PATH_TO_DISK = '/content/drive'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRjNOmM6iDZL",
        "outputId": "c265ff2c-3815-49ce-fe65-1a83ba03669d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(PATH_TO_DISK)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFAlEAMkg3c"
      },
      "source": [
        "## Download pretrained model and pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97EEW0KGk-UC"
      },
      "source": [
        "PATH_TO_MODEL = os.path.join(PATH_TO_DISK, 'My Drive', 'Colab Notebooks/15_epoch_simple_lstm_100.pt')\n",
        "\n",
        "PATH_TO_PRETRAINED_TOKENIZER = 'sberbank-ai/sbert_large_nlu_ru'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zAdGmwbqBN-"
      },
      "source": [
        "# Загрузить sbertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "sbertTokenizer = AutoTokenizer.from_pretrained(PATH_TO_PRETRAINED_TOKENIZER)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH0zU9FMkmoD"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Класс дистиллированной модели-ученика, которую мы будем файнтюнить.\n",
        "# Нужно скопировать из тетрадки, в которой модель дистиллировалась\n",
        "class SimpleLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, batch_size, device=None):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.device = self.init_device(device)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    @staticmethod\n",
        "    def init_device(device):\n",
        "        if device is None:\n",
        "            return torch.device('cuda')\n",
        "        return device\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.zeros(2 * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)),\n",
        "                Variable(torch.zeros(2 * self.n_layers, self.batch_size, self.hidden_dim).to(self.device)))\n",
        "\n",
        "    def forward(self, text, text_lengths=None):\n",
        "        self.hidden = self.init_hidden()\n",
        "        x = self.embedding(text)\n",
        "        x, self.hidden = self.rnn(x, self.hidden)\n",
        "        hidden, cell = self.hidden\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        x = self.fc(hidden)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMmOvpibb9Kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ef4104-3f45-4a27-d6fd-579972fa2ea3"
      },
      "source": [
        "# Загружаем дистиллированную модель. Все параметры должны быть перенесены из тетрадки с дистилляцией.\n",
        "modelParameters = {'input_dim' : sbertTokenizer.vocab_size,\n",
        "                   'embedding_dim' : 50,\n",
        "                   'hidden_dim' : 256,\n",
        "                   'output_dim' : 1024,\n",
        "                   'n_layers' : 2,\n",
        "                   'bidirectional' : True,\n",
        "                   'dropout' : 0.2,\n",
        "                   'batch_size' : 32}\n",
        "\n",
        "model = SimpleLSTM(\n",
        "            input_dim = modelParameters['input_dim'],\n",
        "            embedding_dim = modelParameters['embedding_dim'],\n",
        "            hidden_dim = modelParameters['hidden_dim'],\n",
        "            output_dim = modelParameters['output_dim'],\n",
        "            n_layers = modelParameters['n_layers'],\n",
        "            bidirectional = modelParameters['bidirectional'],\n",
        "            dropout = modelParameters['dropout'],\n",
        "            batch_size = modelParameters['batch_size'])\n",
        "\n",
        "model.load_state_dict(torch.load(PATH_TO_MODEL))\n",
        "\n",
        "# Переносим модель на GPU\n",
        "model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleLSTM(\n",
              "  (embedding): Embedding(120138, 50)\n",
              "  (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDz1sPxbcDXX",
        "outputId": "d61d801a-66b8-45ad-80d4-b9bd5a05f83c"
      },
      "source": [
        "# Проверим, что модель правильно загрузилась\n",
        "tokenized_sent = sbertTokenizer(['Инфляция - жуткая штука'] *32 , padding=True, truncation=True, max_length=20)\n",
        "inds = torch.tensor(tokenized_sent['input_ids'])\n",
        "inds_cuda = inds.to(device)\n",
        "model.eval()\n",
        "model(inds_cuda.t(), None)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        ...,\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997],\n",
              "        [ 0.9378, -0.1869, -0.5879,  ...,  0.1863,  0.5791,  0.5997]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L7o03DHWjzE"
      },
      "source": [
        "# Prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3Ur99khiRAC"
      },
      "source": [
        "# Путь к файлу с данными в .csv формате.\n",
        "# Здесь мы используем датасет Lenta.ru\n",
        "PATH_TO_DATA = os.path.join(PATH_TO_DISK, 'My Drive', 'Colab Notebooks/test-lenta.csv')\n",
        "\n",
        "# Если данные в .zip файле, предварительно их нужно распаковать и указать путь на распакованный .csv-файл\n",
        "# !unzip -o PATH_TO_DATA\n",
        "# PATH_TO_DATA = ...\n",
        "\n",
        "TOKENIZER_MAX_LENGHT = 20"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0nrDS8pm8IK"
      },
      "source": [
        "def getPreparedDataFromCSV(path, max_lenght):\n",
        "    '''\n",
        "    Принимает:  path - путь к .csv файлу с данными\n",
        "                max_lenght - параметр для токенизатора\n",
        "    Выдает:     input_ids - токенизированные предложения, torch.tensor\n",
        "                labels - класссы каждого предложения, которые мы должны научиться предсказывать, torch.tensor\n",
        "                id_to_topic - соответствие классов и реальных меток, list\n",
        "    '''\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    topic_to_id = {}\n",
        "    id_to_topic = []\n",
        "    for topic in data.topic:\n",
        "        if not topic in topic_to_id:\n",
        "            topic_to_id[topic] = len(id_to_topic)\n",
        "            id_to_topic.append(topic)\n",
        "\n",
        "    labels = []\n",
        "    for topic in data.topic:\n",
        "        labels.append(topic_to_id[topic])\n",
        "    \n",
        "    input_ids = [] # encoded sentences\n",
        "\n",
        "    for sent in data.title:\n",
        "        encoded_input = sbertTokenizer(sent, padding='max_length',\n",
        "                                       truncation=True,\n",
        "                                       max_length=max_lenght,\n",
        "                                       return_tensors='pt')\n",
        "        input_ids.append(encoded_input['input_ids'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, labels, id_to_topic"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5NyQViWANnj",
        "outputId": "538f1005-862d-4d36-fa0f-c2d07624472c"
      },
      "source": [
        "# Прочитаем данные из файла и рандомно разобьем их на датасеты\n",
        "# в отношении 7 : 2 : 1 для train : validation : test\n",
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "input_ids, labels, id_to_topic = getPreparedDataFromCSV(PATH_TO_DATA, TOKENIZER_MAX_LENGHT)\n",
        "dataset = TensorDataset(input_ids, labels)\n",
        "\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} testing samples'.format(test_size))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35,000 training samples\n",
            "10,000 validation samples\n",
            "5,000 testing samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkk3y_z_Fsep"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyc8ho17gdNv"
      },
      "source": [
        "trainingParameters = {\n",
        "    'optimizer' : {\n",
        "        'lr' : 2e-5,\n",
        "        'eps' : 1e-8\n",
        "    },\n",
        "    'epochs' : 40,\n",
        "    'batch_size' : modelParameters['batch_size']\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oFeaUsBuXh3"
      },
      "source": [
        "# Создадим DataLoader для каждого датасета\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = trainingParameters['batch_size']\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "testing_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = SequentialSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBILkBfKGLBe"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = trainingParameters['optimizer']['lr'],\n",
        "                  eps = trainingParameters['optimizer']['eps']\n",
        "                )\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tz1Pej3HwYZ"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = trainingParameters['epochs']\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r3Ja7qzIJLn"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwFf7BgJLnIA"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgqnctvOmgyC"
      },
      "source": [
        "# Инициализируем модель, которую будем тренировать\n",
        "# Здесь можно поиграть с количеством слоев и внутренних параметров\n",
        "# Я пробовала один линейный слой и число внутренних параметров (512, 512), (1024, 512), (512, 128)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.model = model\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.lin = nn.Linear(1024, len(id_to_topic))\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x, text_lengths=None):\n",
        "        x = self.model(x.t(), text_lengths=text_lengths).squeeze(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lin(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD3iBnkOq14k",
        "outputId": "c35b9954-de29-4e0d-886e-009930203cb4"
      },
      "source": [
        "net = Net()\n",
        "net.to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (model): SimpleLSTM(\n",
              "    (embedding): Embedding(120138, 50)\n",
              "    (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lin): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syqaDKOELyKa",
        "outputId": "3e0507dd-d18a-444e-dfa1-8c287682d9ff"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# We'll store validation loss, validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print()\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if len(batch[0]) != batch_size:\n",
        "            continue\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_labels = batch[1].cuda()\n",
        "\n",
        "        net.zero_grad()\n",
        "        optimizer.zero_grad()  \n",
        "\n",
        "        result = net(b_input_ids)\n",
        "\n",
        "        loss = criterion(result, b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        del b_input_ids\n",
        "        del b_labels\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        if len(batch[0]) != batch_size:\n",
        "            continue\n",
        "    \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[1].to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            result = net(b_input_ids)\n",
        "\n",
        "        loss = criterion(result, b_labels)\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        _, predicted = torch.max(result.data, 1)\n",
        "        total += b_labels.size(0)\n",
        "        correct += (predicted == b_labels).sum().item()\n",
        "\n",
        "        del b_input_ids\n",
        "        del b_labels\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "\n",
        "\n",
        "    avg_val_accuracy = correct / total\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 40 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 2.25\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.42\n",
            "  Validation Loss: 2.10\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 2.05\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.99\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.98\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.93\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.90\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.90\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation Loss: 1.88\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.88\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.86\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.86\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation Loss: 1.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.85\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.84\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.83\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.84\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.82\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.83\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 11 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.81\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Validation Loss: 1.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 12 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:14.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.81\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Validation Loss: 1.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 13 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Validation Loss: 1.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 14 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.80\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 15 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.79\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 16 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.78\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 17 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.78\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 18 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.78\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 19 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.77\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 20 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.77\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 21 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.76\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 22 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.76\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 23 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.76\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 24 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.76\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 25 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 26 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 27 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 28 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 29 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.75\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 30 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 31 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 32 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 33 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 34 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 35 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 36 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 37 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:00.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 38 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 39 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:14.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 40 / 40 ========\n",
            "Training...\n",
            "  Batch    40  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch    80  of  1,094.    Elapsed: 0:00:01.\n",
            "  Batch   120  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   160  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   200  of  1,094.    Elapsed: 0:00:02.\n",
            "  Batch   240  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   280  of  1,094.    Elapsed: 0:00:03.\n",
            "  Batch   320  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   360  of  1,094.    Elapsed: 0:00:04.\n",
            "  Batch   400  of  1,094.    Elapsed: 0:00:05.\n",
            "  Batch   440  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   480  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   520  of  1,094.    Elapsed: 0:00:06.\n",
            "  Batch   560  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   600  of  1,094.    Elapsed: 0:00:07.\n",
            "  Batch   640  of  1,094.    Elapsed: 0:00:08.\n",
            "  Batch   680  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   720  of  1,094.    Elapsed: 0:00:09.\n",
            "  Batch   760  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   800  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   840  of  1,094.    Elapsed: 0:00:10.\n",
            "  Batch   880  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   920  of  1,094.    Elapsed: 0:00:11.\n",
            "  Batch   960  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,000  of  1,094.    Elapsed: 0:00:12.\n",
            "  Batch 1,040  of  1,094.    Elapsed: 0:00:13.\n",
            "  Batch 1,080  of  1,094.    Elapsed: 0:00:13.\n",
            "\n",
            "  Average training loss: 1.74\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:09:45 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2mr9A_Gb1qz"
      },
      "source": [
        "## Display statictics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m8qFEazDNFXv",
        "outputId": "f47d549a-cbf9-4579-de92-2cca2a78e43c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "df = df_stats.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "df_stats"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.25</td>\n",
              "      <td>2.10</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.05</td>\n",
              "      <td>1.99</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.98</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.93</td>\n",
              "      <td>1.90</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.90</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.88</td>\n",
              "      <td>1.86</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.86</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.85</td>\n",
              "      <td>1.84</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.83</td>\n",
              "      <td>1.84</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.82</td>\n",
              "      <td>1.83</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.81</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.81</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.80</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.80</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.79</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.78</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.78</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.78</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.77</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.77</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.76</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.76</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.76</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.76</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0:00:14</td>\n",
              "      <td>0:00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               2.25         2.10           0.42       0:00:14         0:00:01\n",
              "2               2.05         1.99           0.48       0:00:14         0:00:01\n",
              "3               1.98         1.94           0.53       0:00:14         0:00:01\n",
              "4               1.93         1.90           0.57       0:00:14         0:00:01\n",
              "5               1.90         1.88           0.59       0:00:14         0:00:01\n",
              "6               1.88         1.86           0.61       0:00:14         0:00:01\n",
              "7               1.86         1.85           0.61       0:00:14         0:00:01\n",
              "8               1.85         1.84           0.62       0:00:14         0:00:01\n",
              "9               1.83         1.84           0.63       0:00:14         0:00:01\n",
              "10              1.82         1.83           0.63       0:00:14         0:00:01\n",
              "11              1.81         1.82           0.64       0:00:14         0:00:01\n",
              "12              1.81         1.82           0.64       0:00:14         0:00:01\n",
              "13              1.80         1.82           0.64       0:00:14         0:00:01\n",
              "14              1.80         1.81           0.65       0:00:14         0:00:01\n",
              "15              1.79         1.81           0.65       0:00:14         0:00:01\n",
              "16              1.78         1.81           0.65       0:00:14         0:00:01\n",
              "17              1.78         1.81           0.65       0:00:14         0:00:01\n",
              "18              1.78         1.80           0.65       0:00:14         0:00:01\n",
              "19              1.77         1.80           0.65       0:00:14         0:00:01\n",
              "20              1.77         1.80           0.66       0:00:14         0:00:01\n",
              "21              1.76         1.80           0.66       0:00:14         0:00:01\n",
              "22              1.76         1.80           0.66       0:00:14         0:00:01\n",
              "23              1.76         1.80           0.66       0:00:14         0:00:01\n",
              "24              1.76         1.80           0.66       0:00:14         0:00:01\n",
              "25              1.75         1.79           0.66       0:00:14         0:00:01\n",
              "26              1.75         1.79           0.66       0:00:14         0:00:01\n",
              "27              1.75         1.79           0.66       0:00:14         0:00:01\n",
              "28              1.75         1.79           0.66       0:00:14         0:00:01\n",
              "29              1.75         1.79           0.66       0:00:14         0:00:01\n",
              "30              1.74         1.79           0.66       0:00:14         0:00:01\n",
              "31              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "32              1.74         1.79           0.66       0:00:14         0:00:01\n",
              "33              1.74         1.79           0.66       0:00:14         0:00:01\n",
              "34              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "35              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "36              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "37              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "38              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "39              1.74         1.79           0.67       0:00:14         0:00:01\n",
              "40              1.74         1.79           0.67       0:00:14         0:00:01"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CutE55T6L4gr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "7ff3f34b-143b-42b0-ae0b-d7d07ceaa461"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNZ1BkfrK7PZ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDZlNbOKNeFd",
        "outputId": "6de64f38-bc3a-4a6a-f5fd-4d2f3686de81"
      },
      "source": [
        "net.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "print()\n",
        "print('Testing...')\n",
        "\n",
        "for (step, batch) in enumerate(testing_dataloader):\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    if len(batch[0]) != batch_size:\n",
        "            continue\n",
        "    \n",
        "    if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(testing_dataloader)))\n",
        "\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        result = net(b_input_ids)\n",
        "\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    _, predicted = torch.max(result.data, 1)\n",
        "\n",
        "    predictions.append(predicted)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "    del b_input_ids\n",
        "    del b_labels\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print('DONE.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Testing...\n",
            "  Batch    40  of    157.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    80  of    157.\n",
            "  Batch   120  of    157.\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDb2mTW3Ufm3",
        "outputId": "28045ce5-20b0-4bc7-c93e-7920c587281c"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "predicted_labels = np.concatenate([np.array(prediction.to('cpu'), dtype='long') for prediction in predictions])\n",
        "\n",
        "y_true = np.concatenate(true_labels).flatten()\n",
        "y_pred = predicted_labels.flatten()\n",
        "\n",
        "print(\"Precision: {0:.4f}\".format(precision_score(y_true, y_pred, average='macro'))) # unweighted average of precisions\n",
        "print(\"Recall: {0:.4f}\".format(recall_score(y_true, y_pred, average='macro'))) # unweighted average of recalls\n",
        "print(\"F1-score: {0:.4f}\".format(f1_score(y_true, y_pred, average='macro'))) # unweighted average of f1-scores"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.6643\n",
            "Recall: 0.6620\n",
            "F1-score: 0.6626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugSLY13qWJ55"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltf0COQohwRW"
      },
      "source": [
        "OUTPUT_DIR = os.path.join(PATH_TO_DISK, 'My Drive')\n",
        "MODEL_NAME_TO_SAVE = 'distilled-model'\n",
        "PARAMETERS_NAME_TO_SAVE = 'args-distilled-model.bin'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X4DxhvNRcfb"
      },
      "source": [
        "# Save model, tokenizer and training arguments\n",
        "torch.save(net.state_dict(), os.path.join(OUTPUT_DIR, MODEL_NAME_TO_SAVE))\n",
        "sbertTokenizer.save_pretrained(OUTPUT_DIR)\n",
        "torch.save(trainingParameters, os.path.join(OUTPUT_DIR, PARAMETERS_NAME_TO_SAVE))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNekoZXbvQU"
      },
      "source": [
        "Look at the sizes, out of curiousity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY8TekbTWsbn",
        "outputId": "18c8a212-5d8f-4883-bb6d-8733d93c4dbd"
      },
      "source": [
        "output_dir = './model_save'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "torch.save(net.state_dict(), os.path.join(output_dir, MODEL_NAME_TO_SAVE))\n",
        "sbertTokenizer.save_pretrained(output_dir)\n",
        "torch.save(trainingParameters, os.path.join(output_dir, PARAMETERS_NAME_TO_SAVE))\n",
        "\n",
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 38632K\n",
            "-rw-r--r-- 1 root root     1K May 29 15:51 args-distilled-model.bin\n",
            "-rw-r--r-- 1 root root 34185K May 29 15:51 distilled-model\n",
            "-rw-r--r-- 1 root root     1K May 29 15:51 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root     1K May 29 15:51 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  2689K May 29 15:51 tokenizer.json\n",
            "-rw-r--r-- 1 root root  1739K May 29 15:51 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-I1RKpGXGZT"
      },
      "source": [
        "To load model from drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiYeAnC5SbSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba48731b-4d5c-4c94-8512-2d3a7bb6d380"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Net class must be the same as class of the saved model. Initialize it somewehere before\n",
        "model = Net()\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, MODEL_NAME_TO_SAVE)))\n",
        "tokenizer = BertTokenizerFast.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (model): SimpleLSTM(\n",
              "    (embedding): Embedding(120138, 50)\n",
              "    (rnn): LSTM(50, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lin): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8f5WYyr84u6"
      },
      "source": [
        "def predict(model, tokenizer, sent, id_to_topic):\n",
        "        tokenized_sent = tokenizer([sent] * 32, padding=True, truncation=True, max_length=20)\n",
        "        inds = torch.tensor(tokenized_sent['input_ids'])\n",
        "        inds_cuda = inds.to(device)\n",
        "        model.eval()\n",
        "        model_output = model(inds_cuda, None)[0]\n",
        "        _, predicted = torch.max(model_output, 0)\n",
        "        return id_to_topic[predicted]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "AIO-5uVn94UU",
        "outputId": "a17eb92d-eabc-4baf-fefa-c1b1d24dbfaf"
      },
      "source": [
        "predict(model, tokenizer, 'Инфляция - жуткая штука', id_to_topic)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Экономика'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8c3T1gFcwSS"
      },
      "source": [
        "Done :)"
      ]
    }
  ]
}